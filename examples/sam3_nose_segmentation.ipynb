{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image segmentation with SAM 3\n",
    "\n",
    "This notebook demonstrates how to use SAM 3 for image segmentation with text or visual prompts. It covers the following capabilities:\n",
    "\n",
    "- **Text prompts**: Using natural language descriptions to segment objects (e.g., \"person\", \"face\")\n",
    "- **Box prompts**: Using bounding boxes as exemplar visual prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "output": {
     "id": 1389103906143178,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, \"/home/groups/sammer/haogeh/util/models/sam3/\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sam3\n",
    "from PIL import Image\n",
    "from sam3 import build_sam3_image_model\n",
    "from sam3.model.box_ops import box_xywh_to_cxcywh\n",
    "from sam3.model.sam3_image_processor import Sam3Processor\n",
    "from sam3.visualization_utils import draw_box_on_image, normalize_bbox, plot_results\n",
    "\n",
    "sam3_root = os.path.join(os.path.dirname(sam3.__file__), \"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(sam3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# turn on tfloat32 for Ampere GPUs\n",
    "# https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "\n",
    "# use bfloat16 for the entire notebook\n",
    "torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "output": {
     "id": 785101161160169,
     "loadingStatus": "loaded"
    }
   },
   "outputs": [],
   "source": [
    "bpe_path = f\"{sam3_root}/assets/bpe_simple_vocab_16e6.txt.gz\"\n",
    "model = build_sam3_image_model(bpe_path=bpe_path,checkpoint_path = f'{sam3_root}/assets/checkpoint/sam3.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = f'{sam3_root}/assets/images/dataset_basal_view_consented'\n",
    "bbox_file = os.path.join(dataset_path,\"df_with_bbox.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbox_df = pd.read_pickle(bbox_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xyxy_to_xywh(bbox):\n",
    "    \"\"\"\n",
    "    Convert a bounding box from (x1, y1, x2, y2) format to (x, y, w, h) format.\n",
    "    (x, y) is the top-left corner, (w, h) is width and height.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    x = x1\n",
    "    y = y1\n",
    "    w = x2 - x1\n",
    "    h = y2 - y1\n",
    "    return (x, y, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,line in bbox_df.iterrows():\n",
    "    image_path = os.path.join(dataset_path,line['save_path_rel'])\n",
    "    result_image_path = os.path.join(os.path.dirname(image_path), os.path.basename(image_path).split('.')[0] + '_result.jpg')\n",
    "    mask_path = os.path.join(os.path.dirname(image_path), os.path.basename(image_path).split('.')[0] + '_mask.npz')\n",
    "    # load: state = np.load(\"state.npz\", allow_pickle=True)\n",
    "\n",
    "    image = Image.open(image_path)\n",
    "\n",
    "\n",
    "    width, height = image.size\n",
    "    processor = Sam3Processor(model, confidence_threshold=0.5)\n",
    "\n",
    "    bbox = line['bbox']\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    x,y,w,h = xyxy_to_xywh(bbox)\n",
    "    box_input_xywh = torch.tensor([x,y,w,h]).view(-1, 4)\n",
    "    box_input_cxcywh = box_xywh_to_cxcywh(box_input_xywh)\n",
    "    norm_box_cxcywh = normalize_bbox(box_input_cxcywh, width, height).flatten().tolist()\n",
    "\n",
    "    inference_state = processor.set_image(image)\n",
    "    processor.reset_all_prompts(inference_state)\n",
    "    inference_state = processor.set_text_prompt(state=inference_state, prompt=\"nose from basal view\")\n",
    "    inference_state = processor.add_geometric_prompt(\n",
    "        state=inference_state, box=norm_box_cxcywh, label=True\n",
    "    )\n",
    "\n",
    "    mask = inference_state['masks'].cpu().numpy()[0,0]\n",
    "    mask_logit = inference_state['masks_logits'].cpu().numpy()[0,0]\n",
    "    score = inference_state['scores'].float().cpu().numpy()[0]\n",
    "\n",
    "    save_dict = {}\n",
    "    save_dict['bbox_xyxy'] = x1,y1,x2,y2\n",
    "    save_dict['bbox_xywh'] = x,y,w,h\n",
    "    save_dict['mask'] = mask\n",
    "    save_dict['mask_logit'] = mask_logit\n",
    "\n",
    "    plt.imshow(image)\n",
    "    rect = plt.Rectangle((x1, y1), x2 - x1, y2 - y1, edgecolor='cyan', facecolor='none', linewidth=1)\n",
    "    plt.gca().add_patch(rect)\n",
    "\n",
    "    mask_np = mask.astype(bool)     # ensure boolean\n",
    "    overlay = np.zeros((mask_np.shape[0], mask_np.shape[1], 4), dtype=float)\n",
    "    overlay[mask_np] = [0.0, 1.0, 0.0, 0.5]   # ONLY True pixels get color\n",
    "    plt.imshow(overlay)\n",
    "\n",
    "    plt.title(f\"MRN: {line['mrn']}\")\n",
    "    plt.axis('off')\n",
    "    # plt.show()\n",
    "    plt.savefig(result_image_path,bbox_inches='tight', pad_inches=2)    \n",
    "    np.savez(mask_path,\n",
    "         **{k: v.cpu().numpy() if isinstance(v, torch.Tensor) else v\n",
    "            for k, v in save_dict.items()})\n",
    "    plt.close()\n",
    "\n",
    "    del inference_state\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "fileHeader": "",
  "fileUid": "ff722971-ca5d-431f-8450-ccfea4ff0708",
  "isAdHoc": false,
  "kernelspec": {
   "display_name": "sam3",
   "language": "python",
   "name": "sam3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
